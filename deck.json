{
  "__type__": "Deck",
  "children": [],
  "crowdanki_uuid": "nlp-module4-deck-uuid-2026",
  "deck_config_uuid": "nlp-default-config-uuid",
  "deck_configurations": [
    {
      "__type__": "DeckConfig",
      "crowdanki_uuid": "nlp-default-config-uuid",
      "name": "NLP Default",
      "new": { "perDay": 20, "delays": [1, 10], "ints": [1, 4, 7], "initialFactor": 2500, "bury": true, "separate": true },
      "rev": { "perDay": 100, "ease4": 1.3, "fuzz": 0.05, "maxIvl": 36500, "bury": true, "minSpace": 1, "ivlFct": 1.0 }
    }
  ],
  "desc": "NLP Module 4: Neural Language Models, Encoder-Decoders, and RNNs",
  "media_files": [],
  "name": "NLP::Module 4",
  "note_models": [
    {
      "__type__": "NoteModel",
      "crowdanki_uuid": "cloze-model-nlp-uuid",
      "name": "Cloze-NLP",
      "type": 1,
      "css": ".card {\n font-family: arial;\n font-size: 20px;\n text-align: center;\n color: black;\n background-color: white;\n}\n.cloze {\n font-weight: bold;\n color: blue;\n}",
      "flds": [
        { "name": "Text", "ord": 0, "font": "Arial", "size": 20, "sticky": false, "rtl": false, "media": [] },
        { "name": "Extra", "ord": 1, "font": "Arial", "size": 20, "sticky": false, "rtl": false, "media": [] }
      ],
      "tmpls": [
        {
          "name": "Cloze",
          "ord": 0,
          "qfmt": "{{cloze:Text}}",
          "afmt": "{{cloze:Text}}<br><hr id=answer>{{Extra}}",
          "bfont": "Arial",
          "bsize": 12,
          "bqfmt": "",
          "bafmt": ""
        }
      ],
      "req": [[0, "all", [0]]]
    }
  ],
  "notes": [
    { "__type__": "Note", "fields": ["{{c1::One-Hot Vector}}: A vector of vocabulary size where one bit is 1 and all others are 0.", "Used for input, but huge and sparse."], "guid": "nlp_m4_001", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Vocab"] },
    { "__type__": "Note", "fields": ["A {{c1::Token}} is the integer index of a word in the vocabulary.", "Tokens are inputs for Embedding Layers; One-hots are inputs for Linear Layers."], "guid": "nlp_m4_002", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Vocab"] },
    { "__type__": "Note", "fields": ["The {{c1::Hidden State}} acts as a bottleneck, forcing the network to learn {{c2::generalizations}}.", "Synonyms: History, Context Vector."], "guid": "nlp_m4_003", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Architecture"] },
    { "__type__": "Note", "fields": ["The {{c1::Encoder}} compresses the input into a Hidden State; the {{c2::Decoder}} expands it back to vocabulary size.", "Process Flow: One-Hot -> Linear -> Hidden State -> Linear -> Logits."], "guid": "nlp_m4_004", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Encoder-Decoder"] },
    { "__type__": "Note", "fields": ["Use an {{c1::Embedding Layer}} if input is a Token ID; use a {{c2::Linear Layer}} if input is a One-Hot Vector.", "Embedding layers combine Token-to-One-Hot and Linear steps."], "guid": "nlp_m4_005", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Implementation"] },
    { "__type__": "Note", "fields": ["Bigram Neural Model limitation: Increasing context size ($N$-grams) makes the {{c1::input layer}} wider.", "Requires a different architecture for every context length."], "guid": "nlp_m4_006", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Evolution"] },
    { "__type__": "Note", "fields": ["In an RNN, the inputs at time $t$ are the {{c1::current word}} and the {{c2::Hidden State from time $t-1$}}.", "These are concatenated before passing through a Linear Layer."], "guid": "nlp_m4_007", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "RNN"] },
    { "__type__": "Note", "fields": ["{{c1::Argmax}} always picks the highest probability word, while {{c2::Multinomial Sampling}} treats the output as a distribution.", "Argmax can get stuck in loops; Sampling is more fluent but riskier."], "guid": "nlp_m4_008", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Generation"] },
    { "__type__": "Note", "fields": ["{{c1::Temperature ($T$)}} controls risk in sampling: Low $T$ makes the distribution a {{c2::spike}} (behaving like Argmax).", "$T=1$ leaves the distribution unchanged."], "guid": "nlp_m4_009", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Generation"] },
    { "__type__": "Note", "fields": ["The loss function used for training language models is {{c1::Cross Entropy}}.", "It is the negative log loss of the probability."], "guid": "nlp_m4_010", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Training"] },
    { "__type__": "Note", "fields": ["{{c1::Teacher Forcing}}: During training, we use the actual word from the text as the next input, not the model's guess.", "Input 'Word $t-1$' -> calculate loss against 'Word $t$'."], "guid": "nlp_m4_011", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Training"] },
    { "__type__": "Note", "fields": ["At the very first step of an RNN, the Hidden State is initialized as a {{c1::vector of zeros}}.", "There is no previous history to draw from."], "guid": "nlp_m4_012", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Quiz-Gotcha"] },
    { "__type__": "Note", "fields": ["In an RNN, loss is calculated at {{c1::every single time step}}.", "Not just at the end of the sentence."], "guid": "nlp_m4_013", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Training"] },
    { "__type__": "Note", "fields": ["The {{c1::Packing Problem}}: As sequences get longer, old information in the fixed-size hidden state is overwritten.", "This limitation directly leads to the use of LSTMs."], "guid": "nlp_m4_014", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Evolution"] }
  ]
}
