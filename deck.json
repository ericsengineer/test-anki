{
  "__type__": "Deck",
  "children": [
    {
      "__type__": "Deck",
      "name": "NLP::Module 2",
      "crowdanki_uuid": "nlp-m2-subdeck-uuid",
      "deck_config_uuid": "nlp-default-config-uuid",
      "children": [],
      "media_files": [],
      "notes": [
        { "__type__": "Note", "fields": ["{{c1::Marginalization}} (Summing Out) is the process of getting $P(A)$ from a joint table $P(A, B)$ by summing across all values of $B$.", "Concept Check: Use this when you want to 'remove' a Hidden Variable."], "guid": "nlp_m2_001", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Module2", "Probability"] },
        { "__type__": "Note", "fields": ["The {{c1::Product Rule}} (General Law) states that $P(A, B) = {{c2::P(A|B)P(B)}}$.", "Works for all events, even dependent ones. Do not confuse with Independence ($P(A)P(B)$)."], "guid": "nlp_m2_002", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Module2", "Probability"] },
        { "__type__": "Note", "fields": ["In Bayes' Rule, the {{c1::Prior}} $P(Y)$ is belief before evidence, while the {{c2::Posterior}} $P(Y|E)$ is belief after evidence.", "The Likelihood is $P(E|Y)$. Denominator $P(E)$ is used for normalization."], "guid": "nlp_m2_003", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Module2", "Bayes"] }
      ]
    },
    {
      "__type__": "Deck",
      "name": "NLP::Module 3",
      "crowdanki_uuid": "nlp-m3-subdeck-uuid",
      "deck_config_uuid": "nlp-default-config-uuid",
      "children": [],
      "media_files": [],
      "notes": [
        { "__type__": "Note", "fields": ["{{c1::Bag of Words (BoW)}} represents text as a vector of word counts where vector size always equals {{c2::Vocabulary Size ($V$)}}.", "Crucial Property: It destroys Word Order ('Dog bites man' = 'Man bites dog')."], "guid": "nlp_m3_001", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Module3", "Classification"] },
        { "__type__": "Note", "fields": ["Naïve Bayes 'Naïve' Assumption: Words are {{c1::independent}} given the {{c2::label}}.", "Math Fix: We sum logs ($\\sum \\log P$) instead of multiplying probabilities to prevent underflow."], "guid": "nlp_m3_002", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Module3", "Naive-Bayes"] },
        { "__type__": "Note", "fields": ["The classification loss function {{c1::Negative Log Likelihood}} (Cross-Entropy) uses a negative sign because {{c2::logs of decimals are negative}}.", "We need a positive 'Error' value to minimize during Gradient Descent."], "guid": "nlp_m3_003", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Module3", "Training"] }
      ]
    },
    {
      "__type__": "Deck",
      "name": "NLP::Module 4",
      "crowdanki_uuid": "nlp-m4-subdeck-uuid",
      "deck_config_uuid": "nlp-default-config-uuid",
      "children": [],
      "media_files": [],
      "notes": [
        { "__type__": "Note", "fields": ["{{c1::One-Hot Vector}}: A vector of vocabulary size where one bit is 1 and all others are 0.", "Used for input, but huge and sparse."], "guid": "nlp_m4_001", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Vocab"] },
        { "__type__": "Note", "fields": ["A {{c1::Token}} is the integer index of a word in the vocabulary.", "Tokens are inputs for Embedding Layers; One-hots are inputs for Linear Layers."], "guid": "nlp_m4_002", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Vocab"] },
        { "__type__": "Note", "fields": ["The {{c1::Hidden State}} acts as a bottleneck, forcing the network to learn {{c2::generalizations}}.", "Synonyms: History, Context Vector."], "guid": "nlp_m4_003", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Architecture"] },
        { "__type__": "Note", "fields": ["The {{c1::Encoder}} compresses the input into a Hidden State; the {{c2::Decoder}} expands it back to vocabulary size.", "Process Flow: One-Hot -> Linear -> Hidden State -> Linear -> Logits."], "guid": "nlp_m4_004", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Encoder-Decoder"] },
        { "__type__": "Note", "fields": ["In an RNN, the inputs at time $t$ are the {{c1::current word}} and the {{c2::Hidden State from time $t-1$}}.", "These are concatenated before passing through a Linear Layer."], "guid": "nlp_m4_007", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "RNN"] },
        { "__type__": "Note", "fields": ["{{c1::Temperature ($T$)}} controls risk in sampling: Low $T$ makes the distribution a {{c2::spike}}.", "$T=1$ leaves the distribution unchanged."], "guid": "nlp_m4_009", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Generation"] },
        { "__type__": "Note", "fields": ["The {{c1::Packing Problem}}: As sequences get longer, old information in the fixed-size hidden state is overwritten.", "This limitation directly leads to the use of LSTMs."], "guid": "nlp_m4_014", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Evolution"] }
      ]
    }
  ],
  "crowdanki_uuid": "nlp-master-parent-uuid",
  "deck_config_uuid": "nlp-default-config-uuid",
  "deck_configurations": [
    {
      "__type__": "DeckConfig",
      "crowdanki_uuid": "nlp-default-config-uuid",
      "name": "NLP Default",
      "new": { "perDay": 20, "delays": [1, 10], "ints": [1, 4, 7], "initialFactor": 2500, "bury": true, "separate": true },
      "rev": { "perDay": 100, "ease4": 1.3, "fuzz": 0.05, "maxIvl": 36500, "bury": true, "minSpace": 1, "ivlFct": 1.0 }
    }
  ],
  "desc": "NLP Exam Prep: Foundations, Classification, and Neural Models",
  "media_files": [],
  "name": "NLP",
  "note_models": [
    {
      "__type__": "NoteModel",
      "crowdanki_uuid": "cloze-model-nlp-uuid",
      "name": "Cloze-NLP",
      "type": 1,
      "css": ".card {\n font-family: arial;\n font-size: 20px;\n text-align: center;\n color: black;\n background-color: white;\n}\n.cloze {\n font-weight: bold;\n color: blue;\n}",
      "flds": [
        { "name": "Text", "ord": 0, "font": "Arial", "size": 20, "sticky": false, "rtl": false, "media": [] },
        { "name": "Extra", "ord": 1, "font": "Arial", "size": 20, "sticky": false, "rtl": false, "media": [] }
      ],
      "tmpls": [
        {
          "name": "Cloze",
          "ord": 0,
          "qfmt": "{{cloze:Text}}",
          "afmt": "{{cloze:Text}}<br><hr id=answer>{{Extra}}",
          "bfont": "Arial",
          "bsize": 12,
          "bqfmt": "",
          "bafmt": ""
        }
      ],
      "req": [[0, "all", [0]]]
    }
  ],
<<<<<<< HEAD
  "notes": []
=======
  "notes": [
    { "__type__": "Note", "fields": ["{{c1::One-Hot Vector}}: A vector of vocabulary size where one bit is 1 and all others are 0.", "Used for input, but huge and sparse."], "guid": "nlp_m4_001", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Vocab"] },
    { "__type__": "Note", "fields": ["A {{c1::Token}} is the integer index of a word in the vocabulary.", "Tokens are inputs for Embedding Layers; One-hots are inputs for Linear Layers."], "guid": "nlp_m4_002", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Vocab"] },
    { "__type__": "Note", "fields": ["The {{c1::Hidden State}} acts as a bottleneck, forcing the network to learn {{c2::generalizations}}.", "Synonyms: History, Context Vector."], "guid": "nlp_m4_003", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Architecture"] },
    { "__type__": "Note", "fields": ["The {{c1::Encoder}} compresses the input into a Hidden State; the {{c2::Decoder}} expands it back to vocabulary size.", "Process Flow: One-Hot -> Linear -> Hidden State -> Linear -> Logits."], "guid": "nlp_m4_004", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Encoder-Decoder"] },
    { "__type__": "Note", "fields": ["Use an {{c1::Embedding Layer}} if input is a Token ID; use a {{c2::Linear Layer}} if input is a One-Hot Vector.", "Embedding layers combine Token-to-One-Hot and Linear steps."], "guid": "nlp_m4_005", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Implementation"] },
    { "__type__": "Note", "fields": ["Bigram Neural Model limitation: Increasing context size ($N$-grams) makes the {{c1::input layer}} wider.", "Requires a different architecture for every context length."], "guid": "nlp_m4_006", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Evolution"] },
    { "__type__": "Note", "fields": ["In an RNN, the inputs at time $t$ are the {{c1::current word}} and the {{c2::Hidden State from time $t-1$}}.", "These are concatenated before passing through a Linear Layer."], "guid": "nlp_m4_007", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "RNN"] },
    { "__type__": "Note", "fields": ["{{c1::Argmax}} always picks the highest probability word, while {{c2::Multinomial Sampling}} treats the output as a distribution.", "Argmax can get stuck in loops; Sampling is more fluent but riskier."], "guid": "nlp_m4_008", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Generation"] },
    { "__type__": "Note", "fields": ["{{c1::Temperature ($T$)}} controls risk in sampling: Low $T$ makes the distribution a {{c2::spike}} (behaving like Argmax).", "$T=1$ leaves the distribution unchanged."], "guid": "nlp_m4_009", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Generation"] },
    { "__type__": "Note", "fields": ["The loss function used for training language models is {{c1::Cross Entropy}}.", "It is the negative log loss of the probability."], "guid": "nlp_m4_010", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Training"] },
    { "__type__": "Note", "fields": ["{{c1::Teacher Forcing}}: During training, we use the actual word from the text as the next input, not the model's guess.", "Input 'Word $t-1$' -> calculate loss against 'Word $t$'."], "guid": "nlp_m4_011", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Training"] },
    { "__type__": "Note", "fields": ["At the very first step of an RNN, the Hidden State is initialized as a {{c1::vector of zeros}}.", "There is no previous history to draw from."], "guid": "nlp_m4_012", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Quiz-Gotcha"] },
    { "__type__": "Note", "fields": ["In an RNN, loss is calculated at {{c1::every single time step}}.", "Not just at the end of the sentence."], "guid": "nlp_m4_013", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Training"] },
    { "__type__": "Note", "fields": ["The {{c1::Packing Problem}}: As sequences get longer, old information in the fixed-size hidden state is overwritten.", "This limitation directly leads to the use of LSTMs."], "guid": "nlp_m4_014", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Evolution"] },
    { "__type__": "Note", "fields": ["The **Identity Function** (in autoencoders) attempts to reproduce the {{c1::input}} as the {{c2::output}}.", "Used to learn compression, but not useful for generating new text."], "guid": "nlp_m4_020", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Encoder-Decoder"] },
    { "__type__": "Note", "fields": ["In an RNN loop, the initial hidden state ($h_0$) is typically initialized as a {{c1::vector of zeros}}.", "Before any history exists, there is no context."], "guid": "nlp_m4_021", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "RNN"] },
    { "__type__": "Note", "fields": ["**Teacher Forcing** is a training technique where the input to the next step is the {{c1::ground-truth target word}} rather than the {{c2::model's own prediction}}.", "Prevents errors from compounding during training."], "guid": "nlp_m4_022", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Training"] },
    { "__type__": "Note", "fields": ["**Multinomial Sampling** picks the next word based on its {{c1::probability distribution}}, whereas **Argmax** always picks the {{c2::highest scoring word}}.", "Argmax leads to repetitive loops; Sampling leads to fluency."], "guid": "nlp_m4_023", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Generation"] },
    { "__type__": "Note", "fields": ["In an LSTM, the **Forget Gate** uses a {{c1::Sigmoid}} activation function because it outputs values between {{c2::0 and 1}}.", "0 = Completely remove; 1 = Keep."], "guid": "nlp_m4_030", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "LSTM", "Math"] },
    { "__type__": "Note", "fields": ["In an LSTM, the **Candidate Cell Memory** ($\tilde{C}_t$) uses a {{c1::Tanh}} activation function to output values between {{c2::-1 and 1}}.", "Allows the memory to be strengthened (positive) or weakened (negative)."], "guid": "nlp_m4_031", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "LSTM", "Math"] },
    { "__type__": "Note", "fields": ["The LSTM **Cell State ($C_t$)** represents {{c1::long-term memory (meta-knowledge)}}, while the **Hidden State ($h_t$)** represents {{c2::short-term working memory}}.", "The Cell State is internal; the Hidden State is passed to the output/next layer."], "guid": "nlp_m4_032", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "LSTM"] },
    { "__type__": "Note", "fields": ["The LSTM **Output Gate** determines which parts of the {{c1::Cell State}} are revealed to the {{c2::Hidden State}}.", "It filters the internal memory for immediate use."], "guid": "nlp_m4_033", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "LSTM"] },
    { "__type__": "Note", "fields": ["In a **Seq2Seq** model, the Encoder summarizes the entire input sequence into a single {{c1::Context Vector (Hidden State)}}.", "This creates a bottleneck for long sequences."], "guid": "nlp_m4_040", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Seq2Seq"] },
    { "__type__": "Note", "fields": ["Seq2Seq models are required when the {{c1::input sequence length}} and {{c2::output sequence length}} are different.", "Example: Machine Translation (English to French)."], "guid": "nlp_m4_041", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Seq2Seq"] },
    { "__type__": "Note", "fields": ["In **Attention**, the decoder looks at {{c1::all encoder hidden states}} rather than just the final one.", "It scores them to decide importance."], "guid": "nlp_m4_050", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Attention"] },
    { "__type__": "Note", "fields": ["The Attention mechanism calculates a **weighted sum** of encoder states. The weights are determined by applying {{c1::Softmax}} to the alignment scores.", "Softmax pushes irrelevant scores to near zero."], "guid": "nlp_m4_051", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Attention"] },
    { "__type__": "Note", "fields": ["Mathematically, **Attention** is often described as a {{c1::dot product}} of the softmax vector and the {{c2::candidate matrix}}.", "This is implemented as Batch Matrix Multiplication."], "guid": "nlp_m4_052", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Attention"] },
    { "__type__": "Note", "fields": ["**Perplexity** is defined as the {{c1::exponentiated}} normalized {{c2::cross-entropy loss}}.", "Formula: $PP(W) = e^{H(W)}$"], "guid": "nlp_m4_060", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Evaluation"] },
    { "__type__": "Note", "fields": ["A **Perplexity** of $X$ is equivalent to the difficulty of guessing a value on an {{c1::$X$-sided die}}.", "Lower perplexity is better (less surprised)."], "guid": "nlp_m4_061", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Evaluation"] },
    { "__type__": "Note", "fields": ["**Geometric Mean** is used in Perplexity calculations to normalize for {{c1::sequence length}}.", "Prevents long sentences from having vanishingly small probabilities."], "guid": "nlp_m4_062", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Evaluation"] },
    { "__type__": "Note", "fields": ["**Subword Tokenization** avoids the {{c1::Out of Vocabulary (UNK)}} problem by breaking rare words into common chunks (e.g., prefixes/suffixes).", "Best balance between vocab size and sequence length."], "guid": "nlp_m4_070", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Vocab"] },
    { "__type__": "Note", "fields": ["**Character-level** models have a very small vocabulary but require the LSTM to learn to {{c1::spell}}.", "This puts high stress on the model's memory."], "guid": "nlp_m4_071", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Vocab"] }
  ]
>>>>>>> c4ee771 (add NLP flashcards for LSTM, Seq2Seq, and Attention)
}
