{
  "__type__": "Deck",
  "children": [],
  "crowdanki_uuid": "nlp-module-master-deck-2026",
  "deck_config_uuid": "nlp-default-config-uuid",
  "deck_configurations": [
    {
      "__type__": "DeckConfig",
      "crowdanki_uuid": "nlp-default-config-uuid",
      "name": "NLP Default",
      "new": { "perDay": 20, "delays": [1, 10], "ints": [1, 4, 7], "initialFactor": 2500, "bury": true, "separate": true },
      "rev": { "perDay": 100, "ease4": 1.3, "fuzz": 0.05, "maxIvl": 36500, "bury": true, "minSpace": 1, "ivlFct": 1.0 }
    }
  ],
  "desc": "NLP Exam Prep: Foundations, Classification, and Neural Models",
  "media_files": [],
  "name": "NLP::Master Deck",
  "note_models": [
    {
      "__type__": "NoteModel",
      "crowdanki_uuid": "cloze-model-nlp-uuid",
      "name": "Cloze-NLP",
      "type": 1,
      "css": ".card {\n font-family: arial;\n font-size: 20px;\n text-align: center;\n color: black;\n background-color: white;\n}\n.cloze {\n font-weight: bold;\n color: blue;\n}",
      "flds": [
        { "name": "Text", "ord": 0, "font": "Arial", "size": 20, "sticky": false, "rtl": false, "media": [] },
        { "name": "Extra", "ord": 1, "font": "Arial", "size": 20, "sticky": false, "rtl": false, "media": [] }
      ],
      "tmpls": [
        {
          "name": "Cloze",
          "ord": 0,
          "qfmt": "{{cloze:Text}}",
          "afmt": "{{cloze:Text}}<br><hr id=answer>{{Extra}}",
          "bfont": "Arial",
          "bsize": 12,
          "bqfmt": "",
          "bafmt": ""
        }
      ],
      "req": [[0, "all", [0]]]
    }
  ],
  "notes": [
    { "__type__": "Note", "fields": ["{{c1::Marginalization}} (Summing Out) is the process of getting $P(A)$ from a joint table $P(A, B)$ by summing across all values of $B$.", "Concept Check: Use this when you want to 'remove' a Hidden Variable."], "guid": "nlp_m2_001", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Module2", "Probability"] },
    { "__type__": "Note", "fields": ["The {{c1::Product Rule}} (General Law) states that $P(A, B) = {{c2::P(A|B)P(B)}}$.", "Works for all events, even dependent ones. Do not confuse with Independence ($P(A)P(B)$)."], "guid": "nlp_m2_002", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Module2", "Probability"] },
    { "__type__": "Note", "fields": ["In Bayes' Rule, the {{c1::Prior}} $P(Y)$ is belief before evidence, while the {{c2::Posterior}} $P(Y|E)$ is belief after evidence.", "The Likelihood is $P(E|Y)$. Denominator $P(E)$ is used for normalization."], "guid": "nlp_m2_003", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Module2", "Bayes"] },
    
    { "__type__": "Note", "fields": ["{{c1::Bag of Words (BoW)}} represents text as a vector of word counts where vector size always equals {{c2::Vocabulary Size ($V$)}}.", "Crucial Property: It destroys Word Order ('Dog bites man' = 'Man bites dog')."], "guid": "nlp_m3_001", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Module3", "Classification"] },
    { "__type__": "Note", "fields": ["Naïve Bayes 'Naïve' Assumption: Words are {{c1::independent}} given the {{c2::label}}.", "Math Fix: We sum logs ($\\sum \\log P$) instead of multiplying probabilities to prevent underflow."], "guid": "nlp_m3_002", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Module3", "Naive-Bayes"] },
    { "__type__": "Note", "fields": ["The classification loss function {{c1::Negative Log Likelihood}} (Cross-Entropy) uses a negative sign because {{c2::logs of decimals are negative}}.", "We need a positive 'Error' value to minimize during Gradient Descent."], "guid": "nlp_m3_003", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Module3", "Training"] },

    { "__type__": "Note", "fields": ["{{c1::One-Hot Vector}}: A vector of vocabulary size where one bit is 1 and all others are 0.", "Used for input, but huge and sparse."], "guid": "nlp_m4_001", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Vocab"] },
    { "__type__": "Note", "fields": ["A {{c1::Token}} is the integer index of a word in the vocabulary.", "Tokens are inputs for Embedding Layers; One-hots are inputs for Linear Layers."], "guid": "nlp_m4_002", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Vocab"] },
    { "__type__": "Note", "fields": ["The {{c1::Hidden State}} acts as a bottleneck, forcing the network to learn {{c2::generalizations}}.", "Synonyms: History, Context Vector."], "guid": "nlp_m4_003", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Architecture"] },
    { "__type__": "Note", "fields": ["The {{c1::Encoder}} compresses the input into a Hidden State; the {{c2::Decoder}} expands it back to vocabulary size.", "Process Flow: One-Hot -> Linear -> Hidden State -> Linear -> Logits."], "guid": "nlp_m4_004", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Encoder-Decoder"] },
    { "__type__": "Note", "fields": ["In an RNN, the inputs at time $t$ are the {{c1::current word}} and the {{c2::Hidden State from time $t-1$}}.", "These are concatenated before passing through a Linear Layer."], "guid": "nlp_m4_007", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "RNN"] },
    { "__type__": "Note", "fields": ["{{c1::Temperature ($T$)}} controls risk in sampling: Low $T$ makes the distribution a {{c2::spike}}.", "$T=1$ leaves the distribution unchanged."], "guid": "nlp_m4_009", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Generation"] },
    { "__type__": "Note", "fields": ["The {{c1::Packing Problem}}: As sequences get longer, old information in the fixed-size hidden state is overwritten.", "This limitation directly leads to the use of LSTMs."], "guid": "nlp_m4_014", "note_model_uuid": "cloze-model-nlp-uuid", "tags": ["NLP", "Evolution"] }
  ]
}
